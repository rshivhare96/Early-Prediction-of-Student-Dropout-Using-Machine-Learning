{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset into a Pandas dataframe\n",
    "students_df = pd.read_csv('../data/dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set display width to a larger number\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "# Check the shape of the dataset\n",
    "print(\"\\nDataset shape:\", students_df.shape)\n",
    "\n",
    "# Check the data types of the variables\n",
    "print(\"\\nData types:\")\n",
    "print(students_df.dtypes)\n",
    "\n",
    "# Check the first few rows of the dataset\n",
    "print(\"\\nFirst few rows:\\n\", students_df.head())\n",
    "\n",
    "# Calculate the summary statistics for all columns\n",
    "summary = students_df.describe().transpose().round(2)\n",
    "summary = summary.rename(columns={'50%': 'median', 'std': 'sd'}) # Change 50% to median\n",
    "\n",
    "# Add columns for the number of missing values and the completeness rate\n",
    "summary['n_missing'] = students_df.isnull().sum()\n",
    "summary['complete_rate'] = 1 - (students_df.isnull().sum() / len(students_df))\n",
    "\n",
    "# Print the summary statistics\n",
    "print(\"\\nSummary statistics\\n\",\n",
    "      summary[['mean', 'sd', 'min', '25%', 'median', '75%', 'max', 'n_missing', 'complete_rate']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display number of duplicated rows\n",
    "print(\"Number of duplicated rows: \",students_df.duplicated().sum())\n",
    "\n",
    "# Display rows with duplicates\n",
    "print(students_df[students_df.duplicated()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get no of Unique values in every column\n",
    "for col in students_df.columns:\n",
    "    if len(students_df[col].unique()) < 10:\n",
    "        print(f\"{col} has {len(students_df[col].unique())} unique values; Repeated values are {sorted(students_df[col].unique())}\")\n",
    "    else:\n",
    "        print(f\"{col} has {len(students_df[col].unique())} values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining categorical and quantitative columns based on the above checks and metadata description\n",
    "categorical_cols = [\"Marital status\", \"Application mode\", \"Application order\", \"Course\", \"Daytime/evening attendance\",\n",
    "                    \"Previous qualification\", \"Nacionality\", \"Mother's qualification\", \"Father's qualification\", \"Mother's occupation\",\n",
    "                    \"Father's occupation\", \"Displaced\", \"Educational special needs\", \"Debtor\", \"Tuition fees up to date\", \"Gender\",\n",
    "                    \"Scholarship holder\", \"International\", \"Target\"]\n",
    "\n",
    "quantitative_cols = ['Curricular units 1st sem (credited)', 'Curricular units 1st sem (enrolled)', 'Curricular units 1st sem (evaluations)',\n",
    "                  'Curricular units 1st sem (approved)', 'Curricular units 1st sem (grade)', 'Curricular units 1st sem (without evaluations)',\n",
    "                  'Curricular units 2nd sem (credited)', 'Curricular units 2nd sem (enrolled)', 'Curricular units 2nd sem (evaluations)',\n",
    "                  'Curricular units 2nd sem (approved)', 'Curricular units 2nd sem (grade)', 'Curricular units 2nd sem (without evaluations)',\n",
    "                  'Age at enrollment', 'Inflation rate', 'GDP', 'Unemployment rate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through categorical columns and create barplot for each\n",
    "fig, axes = plt.subplots(nrows=5, ncols=4, figsize=(20, 25))\n",
    "for i, col in enumerate(categorical_cols):\n",
    "    row_idx = i // 4\n",
    "    col_idx = i % 4\n",
    "    vc = students_df[col].value_counts(normalize=True)\n",
    "    vc = vc.sort_index() # sort by category name\n",
    "    vc.plot(kind=\"bar\", ax=axes[row_idx, col_idx], rot=0)\n",
    "    axes[row_idx, col_idx].set_title(col)\n",
    "    for tick in axes[row_idx, col_idx].get_xticklabels():\n",
    "        tick.set_rotation(90)\n",
    "    for tick in axes[row_idx, col_idx].get_yticklabels():\n",
    "        tick.set_visible(False)\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through categorical columns and create frequency table for each\n",
    "for col in categorical_cols:\n",
    "    print(f\"Frequency Table for {col}:\")\n",
    "    print(students_df[col].value_counts(normalize=True))\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histograms for all quantitative columns\n",
    "students_df[quantitative_cols].hist(figsize=(20, 20));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical Summaries for quantitative variables\n",
    "summary_df = students_df[quantitative_cols].describe().loc[['mean', 'std', 'min', '25%', '50%', '75%', 'max']]\n",
    "summary_df.loc['skew'] = students_df[quantitative_cols].skew()\n",
    "\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show numerical summaries for columns of interest i.e. Inflation rate and GDP\n",
    "cols_of_interest = ['Inflation rate', 'GDP']\n",
    "summary_df2 = students_df[cols_of_interest].describe().loc[['mean', 'std', 'min', '25%', '50%', '75%', 'max']]\n",
    "summary_df2.loc['skew'] = students_df[cols_of_interest].skew()\n",
    "print(summary_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show skew values for higher skew variables\n",
    "print(\"Variables with higher skew values:\")\n",
    "for col in quantitative_cols:\n",
    "    if abs(summary_df.loc['skew', col]) > 1:\n",
    "        print(f\"\\t{col}: {summary_df.loc['skew', col]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duplicate the data\n",
    "students_df_cleaned = students_df.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import library to calculate mode\n",
    "from scipy.stats import mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace 0 values with NaN\n",
    "students_df_cleaned['Application order'].replace(0, np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute missing values with mode\n",
    "students_df_cleaned['Application order'].fillna(mode_value, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the minimum frequency for combining categories\n",
    "min_freq = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine categories with low frequency\n",
    "combined_cats_dict = {}\n",
    "for col in categorical_cols:\n",
    "  # Check if the variable is not binary\n",
    "  if len(students_df_cleaned[col].unique()) > 2:\n",
    "    # Count the frequency of each category\n",
    "    counts = students_df_cleaned[col].value_counts()\n",
    "    # Identify the categories with low frequency\n",
    "    low_freq_cats = counts[counts < min_freq].index.tolist()\n",
    "    # Only combine categories if at least 2 categories have frequency less than min_freq\n",
    "    if len(low_freq_cats) >= 2:\n",
    "        # Combine the low frequency categories into a single \"Other\" category represented as -1\n",
    "        students_df_cleaned[col] = students_df_cleaned[col].apply(lambda x: -1 if x in low_freq_cats else x)\n",
    "        # Identify the categories that were combined\n",
    "        combined_cats_dict[col] = ', '.join(str(cat) for cat in low_freq_cats)\n",
    "        other_percent = len(students_df_cleaned[students_df_cleaned[col] == -1]) / len(students_df) * 100\n",
    "        #print(f\"Categories combined for {col}: {combined_cats_dict[col]} \\t totalling: {other_percent:.2f}%\")\n",
    "        print(f\"For {col} variable, -1 (Other) constitutes {other_percent:.2f}% & combines categories {combined_cats_dict[col]}\")\n",
    "\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through categorical columns and create frequency table for each\n",
    "for col in categorical_cols:\n",
    "    # Create a frequency table for the current categorical column\n",
    "    freq_table = students_df_cleaned[col].value_counts(normalize=True).mul(100).round(2).astype(str) + '%'\n",
    "    print(f\"Frequency Table for {col}:\")\n",
    "    print(freq_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change Column Name in dataset\n",
    "students_df_cleaned = students_df_cleaned.rename(columns={'Nacionality': 'Nationality'})\n",
    "\n",
    "# Update the column name in the categorical_cols list\n",
    "categorical_cols[categorical_cols.index('Nacionality')] = 'Nationality'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify columns with high skewness\n",
    "high_skew_cols = summary_df.columns[(summary_df.loc['skew'] > 1) | (summary_df.loc['skew'] < -1)]\n",
    "\n",
    "# Duplicate the dataset\n",
    "students_df_normalised = students_df_cleaned.copy()\n",
    "\n",
    "# Normalize data using log transformation\n",
    "students_df_normalised[high_skew_cols] = np.sqrt(students_df_normalised[high_skew_cols])\n",
    "\n",
    "# Alternatively, we can also use other normalization techniques\n",
    "#students_df_cleaned[high_skew_cols] = np.log(students_df_cleaned[high_skew_cols])\n",
    "\n",
    "# Numerical Summaries for quantitative variables\n",
    "summary_df_normalised = students_df_normalised[quantitative_cols].describe().loc[['mean', 'std', 'min', '25%', '50%', '75%', 'max']]\n",
    "summary_df_normalised.loc['skew'] = students_df_normalised[quantitative_cols].skew()\n",
    "\n",
    "# Stack the two dataframes vertically with a separator\n",
    "separator = pd.DataFrame([['----'] * len(quantitative_cols)], columns=quantitative_cols)\n",
    "stacked_summary = pd.concat([summary_df, separator, summary_df_normalised], keys=['Original Summary', '----', 'Normalised Summary'])\n",
    "\n",
    "stacked_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy of the original normalized dataset\n",
    "students_df_normalised_no_outliers = students_df_normalised.copy()\n",
    "\n",
    "# Calculate the IQR for each numerical column\n",
    "Q1 = students_df_normalised[quantitative_cols].quantile(0.25)\n",
    "Q3 = students_df_normalised[quantitative_cols].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Define the upper and lower bounds for outlier removal\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Identify and remove outliers\n",
    "outliers = ((students_df_normalised[quantitative_cols] < lower_bound) | (students_df_normalised[quantitative_cols] > upper_bound)).any(axis=1)\n",
    "students_df_normalised_no_outliers = students_df_normalised[~outliers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr_plot(df, title, threshold = 0.5):\n",
    "  # Compute the correlation matrix on cleaned dataset\n",
    "  corr_matrix = df.corr()\n",
    "\n",
    "  # Create a mask to display only the upper triangle of the heatmap\n",
    "  mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "\n",
    "  # Create a heatmap with the high correlation variables annotated\n",
    "  plt.figure(figsize=(10,8))\n",
    "  sns.heatmap(corr_matrix[(corr_matrix > threshold) | (corr_matrix < -threshold)],\n",
    "              cmap=\"coolwarm\", annot=True, annot_kws={\"size\": 7}, mask=mask)\n",
    "  title = \"Heatmap showing correlation in \" + title + \" variables\"\n",
    "  plt.title(title, fontsize=18)\n",
    "  plt.show()\n",
    "\n",
    "  return corr_matrix\n",
    "\n",
    "# Convert Target to numerical\n",
    "students_df_cleaned[\"Target\"].replace('Dropout', 0, inplace=True)\n",
    "students_df_cleaned[\"Target\"].replace('Graduate', 1, inplace=True)\n",
    "students_df_cleaned[\"Target\"].replace('Enrolled', 2, inplace=True)\n",
    "\n",
    "\n",
    "# Demographic Features\n",
    "demographic_df = students_df_cleaned[[\"Marital status\", \"Nationality\", \"Displaced\",\n",
    "                                      \"Gender\", \"Age at enrollment\", \"International\",\n",
    "                                      \"Target\"]]\n",
    "\n",
    "# Socio Economic Features\n",
    "socioeconomic_df = students_df_cleaned[[\"Mother's qualification\", \"Father's qualification\",\n",
    "                                       \"Mother's occupation\", \"Father's occupation\",\n",
    "                                       \"Educational special needs\", \"Debtor\",\n",
    "                                       \"Tuition fees up to date\", \"Scholarship holder\",\n",
    "                                       \"Target\"]]\n",
    "\n",
    "# Macro and Enrollment Features\n",
    "macroenrollment_df = students_df_cleaned[['Unemployment rate', 'Inflation rate', 'GDP', 'Application mode',\n",
    "                                          'Application order', 'Course', 'Daytime/evening attendance',\n",
    "                                          'Previous qualification', 'Target']]\n",
    "\n",
    "# Academic Features\n",
    "academic_df = students_df_cleaned[['Curricular units 1st sem (credited)', 'Curricular units 1st sem (enrolled)',\n",
    "                                         'Curricular units 1st sem (evaluations)', 'Curricular units 1st sem (approved)',\n",
    "                                         'Curricular units 1st sem (grade)', 'Curricular units 1st sem (without evaluations)',\n",
    "                                         'Curricular units 2nd sem (credited)', 'Curricular units 2nd sem (enrolled)',\n",
    "                                         'Curricular units 2nd sem (evaluations)', 'Curricular units 2nd sem (approved)',\n",
    "                                         'Curricular units 2nd sem (grade)', 'Curricular units 2nd sem (without evaluations)',\n",
    "                                         'Target']]\n",
    "\n",
    "# Combining results into one\n",
    "corr_df = []\n",
    "\n",
    "# Plot the correlation heatmaps for each group with correlation threshold 0\n",
    "corr_df.append(corr_plot(demographic_df, \"Demographic\", 0))\n",
    "corr_df.append(corr_plot(socioeconomic_df, \"Socio-Economic\", 0))\n",
    "corr_df.append(corr_plot(macroenrollment_df, \"Macro-Economic and Enrollment\", 0))\n",
    "corr_df.append(corr_plot(academic_df, \"Academic\", 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features to remove\n",
    "features_to_remove = [\"Curricular units 1st sem (credited)\", \"Curricular units 1st sem (enrolled)\",\n",
    "                      \"Curricular units 1st sem (evaluations)\", \"Curricular units 1st sem (approved)\",\n",
    "                      \"Curricular units 1st sem (grade)\", \"Curricular units 2nd sem (approved)\", \"Nationality\"]\n",
    "\n",
    "# remove the specified columns from the dataframe\n",
    "students_df_cleaned = students_df_cleaned.drop(features_to_remove, axis=1)\n",
    "\n",
    "# remove columns from categorical columns\n",
    "categorical_cols = [col for col in categorical_cols if col not in features_to_remove]\n",
    "\n",
    "# remove columns from quantitative columns\n",
    "quantitative_cols = [col for col in quantitative_cols if col not in features_to_remove]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Create a copy of the original dataset\n",
    "students_df_encoded = students_df_cleaned.copy()\n",
    "\n",
    "# Convert the Target variable to numerical representation\n",
    "students_df_encoded['Target'] = students_df_encoded['Target'].astype('category').cat.codes\n",
    "\n",
    "# Compute the correlation matrix\n",
    "corr_matrix = students_df_encoded.corr()\n",
    "'''\n",
    "# Compute the correlation matrix on cleaned dataset\n",
    "corr_matrix = students_df_cleaned.corr()\n",
    "\n",
    "# Set the threshold for high correlation\n",
    "threshold = 0\n",
    "\n",
    "# Create a mask to display only the upper triangle of the heatmap\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "\n",
    "# Create a heatmap with the high correlation variables annotated\n",
    "plt.figure(figsize=(12,8))\n",
    "sns.heatmap(corr_matrix[(corr_matrix > threshold) | (corr_matrix < -threshold)],\n",
    "            cmap=\"coolwarm\", annot=True, annot_kws={\"size\": 6}, mask=mask)\n",
    "\n",
    "# Add lines connecting the highly correlated variables\n",
    "'''\n",
    "for i in range(len(corr_matrix.columns)):\n",
    "    for j in range(i):\n",
    "        if abs(corr_matrix.iloc[i, j]) > threshold:\n",
    "            plt.axhline(y=i+0.5, color='grey', linestyle='dotted', alpha=0.1)\n",
    "            plt.axvline(x=j+0.5, color='grey', linestyle='dotted', alpha=0.1)\n",
    "'''\n",
    "\n",
    "plt.title(\"Correlation Matrix of All Remaining Variables\", fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store high correlations in a list of tuples and sort it by absolute value of the correlation coefficient\n",
    "high_corr = []\n",
    "for i in range(len(corr_matrix.columns)):\n",
    "    for j in range(i):\n",
    "        if abs(corr_matrix.iloc[i, j]) > threshold:\n",
    "            high_corr.append((corr_matrix.columns[i], corr_matrix.columns[j], abs(corr_matrix.iloc[i, j])))\n",
    "high_corr.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "# Print top 10 high correlations in sorted order\n",
    "for c1, c2, corr in high_corr[:10]:\n",
    "    print(f\"{c1} - {c2}: {round(corr,2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing enrolled from the Target variable\n",
    "students_df_cleaned = students_df_cleaned[students_df_cleaned.Target != 2]\n",
    "\n",
    "# Check the distribution of the \"Target\" variable\n",
    "sns.countplot(x='Target', data=students_df_cleaned)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency table for Target\n",
    "print(\"Target distribution\")\n",
    "target_counts = students_df_cleaned['Target'].value_counts()\n",
    "print(\"0\\tDropout \\t\", target_counts[0])\n",
    "print(\"1\\tGraduate\\t\", target_counts[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 9x4 grid of subplots for the bar charts\n",
    "fig, axes = plt.subplots(nrows=9, ncols=4, figsize=(20,45))\n",
    "\n",
    "# Select all categorical columns except Target\n",
    "selected_cols = [col for col in categorical_cols if col != \"Target\"]\n",
    "\n",
    "# Loop through the selected columns and create a bar chart for each one\n",
    "for i in range(len(selected_cols)):\n",
    "    row1 = i//2\n",
    "    col1 = (i % 2) * 2\n",
    "    col2 = col1 + 1\n",
    "    counts = students_df_cleaned.groupby([selected_cols[i], 'Target'])['Course'].count().unstack()\n",
    "    counts_norm = counts.apply(lambda x: x/x.sum(), axis=1)\n",
    "    counts.plot(kind='bar', stacked=True, ax=axes[row1][col1])\n",
    "    axes[row1][col1].set_title(selected_cols[i] + \" Count\", fontsize=12)\n",
    "    axes[row1][col1].set_xlabel('')\n",
    "    axes[row1][col1].set_ylabel('Frequency')\n",
    "    axes[row1][col1].legend(title='Target')\n",
    "\n",
    "    counts_norm.plot(kind='bar', stacked=True, ax=axes[row1][col2])\n",
    "    axes[row1][col2].set_title(selected_cols[i] + \" Proportion\", fontsize=12)\n",
    "    axes[row1][col2].set_xlabel('')\n",
    "    axes[row1][col2].set_ylabel('Proportion')\n",
    "    axes[row1][col2].legend(title='Target')\n",
    "\n",
    "\n",
    "# Adjust the spacing between subplots and display the figure\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a 2x9 grid of subplots for the bar charts (since we only want to show proportions)\n",
    "fig, axes = plt.subplots(nrows=3, ncols=5, figsize=(25,12))\n",
    "\n",
    "# Select all categorical columns except Target\n",
    "selected_cols = [col for col in categorical_cols if col not in [\"Target\", \"International\", \"Educational special needs\"]]\n",
    "\n",
    "# Loop through the selected columns and create a bar chart for proportion in each subplot\n",
    "for i in range(len(selected_cols)):\n",
    "    row = i // 5\n",
    "    col = i % 5\n",
    "    counts = students_df_cleaned.groupby([selected_cols[i], 'Target'])['Course'].count().unstack()\n",
    "    counts_norm = counts.apply(lambda x: x/x.sum(), axis=1)\n",
    "    counts_norm.plot(kind='bar', stacked=True, ax=axes[row][col])\n",
    "    axes[row][col].set_title(selected_cols[i] + \" Proportion\", fontsize=18)\n",
    "    axes[row][col].set_xlabel('')\n",
    "    axes[row][col].set_ylabel('Proportion')\n",
    "    axes[row][col].legend(title='Target')\n",
    "\n",
    "# Adjust the spacing between subplots and display the figure\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through selected columns\n",
    "for col in selected_cols:\n",
    "\n",
    "    # Group data by column and Target, then unstack\n",
    "    freq_df = students_df_cleaned.groupby([col, 'Target']).size().unstack()\n",
    "\n",
    "    # Rename columns and reset index\n",
    "    freq_df.columns = ['Dropout', 'Graduate']\n",
    "    freq_df = freq_df.reset_index()\n",
    "\n",
    "    # Compute total and percentage for each Target\n",
    "    freq_df['Total'] = freq_df['Dropout'] + freq_df['Graduate']\n",
    "    freq_df['Percent Dropout'] = (freq_df['Dropout'] / freq_df['Total'] * 100).round(2).fillna('-').astype(str) + '%'\n",
    "    freq_df['Percent Graduate'] = (freq_df['Graduate'] / freq_df['Total'] * 100).round(2).fillna('-').astype(str) + '%'\n",
    "\n",
    "    # Rearrange and rename columns, fill NaN values with '-'\n",
    "    freq_df = freq_df[[col, 'Graduate', 'Percent Graduate', 'Dropout', 'Percent Dropout']]\n",
    "    freq_df.columns = [col, 'Graduate', 'Percent', 'Dropout', 'Percent']\n",
    "    freq_df = freq_df.fillna('-')\n",
    "\n",
    "    # Print the resulting table\n",
    "    print(freq_df.to_string(index=False), \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 4x4 grid of subplots for the boxplots\n",
    "fig, axes = plt.subplots(nrows=2, ncols=5, figsize=(20,8))\n",
    "\n",
    "# Loop through the quantitative columns and create a boxplot for each one\n",
    "for i in range(len(quantitative_cols)):\n",
    "    row = i // 5\n",
    "    col = i % 5\n",
    "    sns.boxplot(x='Target', y=quantitative_cols[i], data=students_df_cleaned, ax=axes[row][col])\n",
    "\n",
    "# Adjust the spacing between subplots and display the figure\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the data by 'Target' and compute the numerical summary\n",
    "num_summary = students_df_cleaned.groupby('Target')[quantitative_cols].describe().T\n",
    "\n",
    "# Customize the output\n",
    "pd.set_option('display.float_format', lambda x: '%.2f' % x)\n",
    "pd.set_option('display.max_columns', 1000)\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "\n",
    "# Print the numerical summary\n",
    "num_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the distribution of the \"Scholarship holder\" variable\n",
    "sns.countplot(x='Scholarship holder', data=students_df_cleaned)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency table for Scholarship holder\n",
    "students_df_cleaned['Scholarship holder'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 4x4 grid of subplots for the boxplots\n",
    "fig, axes = plt.subplots(nrows=4, ncols=4, figsize=(20,20))\n",
    "\n",
    "# Loop through the quantitative columns and create a boxplot for each one\n",
    "for i in range(len(quantitative_cols)):\n",
    "    row = i // 4\n",
    "    col = i % 4\n",
    "    sns.boxplot(x='Scholarship holder', y=quantitative_cols[i], data=students_df_cleaned, ax=axes[row][col])\n",
    "\n",
    "# Adjust the spacing between subplots and display the figure\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the data by 'Scholarship holder' and compute the numerical summary\n",
    "num_summary = students_df_cleaned.groupby('Scholarship holder')[quantitative_cols].describe().T\n",
    "\n",
    "# Customize the output\n",
    "pd.set_option('display.float_format', lambda x: '%.2f' % x)\n",
    "pd.set_option('display.max_columns', 1000)\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "\n",
    "# Print the numerical summary\n",
    "print(num_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 9x4 grid of subplots for the bar charts\n",
    "fig, axes = plt.subplots(nrows=9, ncols=4, figsize=(20,45))\n",
    "\n",
    "# Select all categorical columns except Scholarship holder\n",
    "selected_cols = [col for col in categorical_cols if col != \"Scholarship holder\"]\n",
    "\n",
    "# Loop through the categorical columns and create a bar chart for each one\n",
    "for i in range(len(selected_cols)):\n",
    "    row1 = i//2\n",
    "    col1 = (i % 2) * 2\n",
    "    col2 = col1 + 1\n",
    "    counts = students_df_cleaned.groupby([selected_cols[i], 'Scholarship holder'])['Course'].count().unstack()\n",
    "    counts_norm = counts.apply(lambda x: x/x.sum(), axis=1)\n",
    "    counts.plot(kind='bar', stacked=True, ax=axes[row1][col1])\n",
    "    axes[row1][col1].set_title(selected_cols[i] + \" Count\", fontsize=12)\n",
    "    axes[row1][col1].set_xlabel('')\n",
    "    axes[row1][col1].set_ylabel('Frequency')\n",
    "    axes[row1][col1].legend(title='Scholarship holder')\n",
    "\n",
    "    counts_norm.plot(kind='bar', stacked=True, ax=axes[row1][col2])\n",
    "    axes[row1][col2].set_title(selected_cols[i] + \" Proportion\", fontsize=12)\n",
    "    axes[row1][col2].set_xlabel('')\n",
    "    axes[row1][col2].set_ylabel('Proportion')\n",
    "    axes[row1][col2].legend(title='Scholarship holder')\n",
    "\n",
    "\n",
    "# Adjust the spacing between subplots and display the figure\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through categorical columns\n",
    "for col in selected_cols:\n",
    "\n",
    "    # Group data by column and Scholarship holder, then unstack\n",
    "    freq_df = students_df_cleaned.groupby([col, 'Scholarship holder']).size().unstack()\n",
    "\n",
    "    # Rename columns and reset index\n",
    "    freq_df.columns = ['No Scholarship', 'Scholarship']\n",
    "    freq_df = freq_df.reset_index()\n",
    "\n",
    "    # Compute total and percentage for each Scholarship holder\n",
    "    freq_df['Total'] = freq_df['No Scholarship'] + freq_df['Scholarship']\n",
    "    freq_df['Percent No Scholarship'] = (freq_df['No Scholarship'] / freq_df['Total'] * 100).round(2).fillna('-').astype(str) + '%'\n",
    "    freq_df['Percent Scholarship'] = (freq_df['Scholarship'] / freq_df['Total'] * 100).round(2).fillna('-').astype(str) + '%'\n",
    "\n",
    "    # Rearrange and rename columns, fill NaN values with '-'\n",
    "    freq_df = freq_df[[col, 'Scholarship', 'Percent Scholarship', 'No Scholarship', 'Percent No Scholarship']]\n",
    "    freq_df.columns = [col, 'Scholarship', 'Percent', 'No Scholarship', 'Percent']\n",
    "    freq_df = freq_df.fillna('-')\n",
    "\n",
    "    # Print the resulting table\n",
    "    print(freq_df.to_string(index=False), \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create a copy of the original dataset\n",
    "num_vars = students_df_cleaned[quantitative_cols].copy()\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "num_vars_scaled = scaler.fit_transform(num_vars)\n",
    "num_vars_scaled = pd.DataFrame(num_vars_scaled, columns=quantitative_cols)\n",
    "\n",
    "# Set up a range of k values to try\n",
    "k_values = range(1, 11)\n",
    "\n",
    "# Calculate the inertia for each value of k\n",
    "inertias = []\n",
    "for k in k_values:\n",
    "    model = KMeans(n_clusters=k, random_state=42,  n_init=10)\n",
    "    model.fit(num_vars)\n",
    "    inertias.append(model.inertia_)\n",
    "\n",
    "# Visualize the elbow plot\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,6))\n",
    "\n",
    "ax1.plot(k_values, inertias, 'bo-')\n",
    "ax1.set_xlabel('Number of Clusters (k)')\n",
    "ax1.set_ylabel('Inertia')\n",
    "ax1.set_title('Elbow Plot')\n",
    "\n",
    "# Calculate the inertia for each value of k\n",
    "inertias = []\n",
    "for k in k_values:\n",
    "    model = KMeans(n_clusters=k, random_state=42,  n_init=10)\n",
    "    model.fit(num_vars_scaled)\n",
    "    inertias.append(model.inertia_)\n",
    "\n",
    "# Visualize the elbow plot\n",
    "ax2.plot(k_values, inertias, 'bo-')\n",
    "ax2.set_xlabel('Number of Clusters (k)')\n",
    "ax2.set_ylabel('Inertia')\n",
    "ax2.set_title('Elbow Plot (with Scaled Vars)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the optimal k values based on the elbow methods above\n",
    "k_optimal_unscaled = 3\n",
    "k_optimal_scaled = 3\n",
    "\n",
    "# Build the unscaled quantitative variables model\n",
    "model_unscaled = KMeans(n_clusters=k_optimal_unscaled, random_state=42, n_init=10)\n",
    "model_unscaled.fit(num_vars)\n",
    "\n",
    "# Build the scaled quantitative variables model\n",
    "model_scaled = KMeans(n_clusters=k_optimal_scaled, random_state=42, n_init=10)\n",
    "model_scaled.fit(num_vars_scaled)\n",
    "\n",
    "# Get the cluster labels for each data point\n",
    "labels_unscaled = model_unscaled.labels_\n",
    "labels_scaled = model_scaled.labels_\n",
    "\n",
    "# Visualize the clusters using scatter points\n",
    "fig, (ax2, ax1) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Unscaled Variables Model\n",
    "ax2.scatter(num_vars_scaled.iloc[:, num_vars.columns.get_loc(\"Age at enrollment\")], num_vars_scaled.iloc[:, num_vars.columns.get_loc(\"Curricular units 2nd sem (credited)\")], c=labels_unscaled)\n",
    "centers = model_unscaled.cluster_centers_\n",
    "ax2.scatter(centers[:, 0], centers[:, 1], c='red', marker='x')\n",
    "ax2.set_title('K-Means Clustering (Unscaled Variables, k=3)', fontsize=14)\n",
    "ax2.set_xlabel('Age at enrollment', fontsize=12)\n",
    "ax2.set_ylabel('Curricular units 2nd sem (credited)', fontsize=12)\n",
    "\n",
    "# Scaled Variables Model\n",
    "ax1.scatter(num_vars_scaled.iloc[:, num_vars.columns.get_loc(\"Age at enrollment\")], num_vars_scaled.iloc[:, num_vars.columns.get_loc(\"Curricular units 2nd sem (credited)\")], c=labels_scaled)\n",
    "centers = model_scaled.cluster_centers_\n",
    "ax1.scatter(centers[:, 0], centers[:, 1], c='red', marker='x')\n",
    "ax1.set_title('K-Means Clustering (Scaled Variables, k=3)', fontsize=14)\n",
    "ax1.set_xlabel('Age at enrollment (Scaled)', fontsize=12)\n",
    "ax1.set_ylabel('Curricular units 2nd sem (credited) (Scaled)', fontsize=12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "## Split the dataset into quantitative variables only\n",
    "\n",
    "# Using all columns except Target, we need 6 PCs to cummulatively explain 80% of the variance\n",
    "#num_vars = students_df.select_dtypes(include=np.number)\n",
    "\n",
    "# Using predefined quantitative columns, we need 3 PCs to cummulatively explain 80% of the variance\n",
    "num_vars = students_df_cleaned[quantitative_cols]\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "num_vars_scaled = scaler.fit_transform(num_vars)\n",
    "\n",
    "# Perform PCA on non-scaled variables\n",
    "pca = PCA()\n",
    "pca.fit(num_vars)\n",
    "\n",
    "# Get the eigenvalues and eigenvectors\n",
    "eigenvalues = pca.explained_variance_\n",
    "eigenvectors = pca.components_\n",
    "\n",
    "# Get the explained variance ratio for each principal component\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "\n",
    "# Create a figure with two subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "# Create a bar graph of the explained variance ratio for each principal component\n",
    "ax1.bar(range(1, len(explained_variance_ratio)+1), explained_variance_ratio)\n",
    "ax1.set_xticks(range(1, len(explained_variance_ratio)+1))\n",
    "ax1.set_xlabel('Principal Component')\n",
    "ax1.set_ylabel('Explained Variance Ratio')\n",
    "ax1.set_title('Explained Variance Ratio for each Principal Component')\n",
    "\n",
    "# Plot the cumulative explained variance ratio\n",
    "cumulative_variance_ratio = np.cumsum(explained_variance_ratio)\n",
    "ax2.plot(range(1, len(cumulative_variance_ratio)+1,1), cumulative_variance_ratio)\n",
    "\n",
    "# add a horizontal line at 80% cutoff\n",
    "ax2.axhline(y=0.8, color='r', linestyle='-')\n",
    "\n",
    "# find the index where the cumulative variance first exceeds 80%\n",
    "variance_80 = next(idx for idx, cum_var in enumerate(pca.explained_variance_ratio_.cumsum()) if cum_var > 0.8)\n",
    "\n",
    "# add a vertical line at the intersection point\n",
    "ax2.axvline(x=variance_80+1, color='r', linestyle='-')\n",
    "\n",
    "ax2.set_title('Cumulative Variance Plot')\n",
    "ax2.set_xlabel('Number of Principal Components')\n",
    "ax2.set_ylabel('Cumulative Explained Variance Ratio')\n",
    "\n",
    "# Adjust the layout and spacing between subplots\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explained_variance_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to store the loadings\n",
    "loadings = {}\n",
    "\n",
    "# Iterate through each principal component\n",
    "for i, pc in enumerate(pca.components_):\n",
    "    # Create a list to store the loadings of each variable\n",
    "    loading_list = []\n",
    "    # Iterate through each variable and its loading\n",
    "    for j, var in enumerate(num_vars.columns):\n",
    "        loading = pc[j]\n",
    "        # Append the variable and its loading to the list\n",
    "        loading_list.append((var, loading))\n",
    "    # Sort the list based on the absolute value of the loading\n",
    "    loading_list.sort(key=lambda x: abs(x[1]), reverse=True)\n",
    "    # Add the sorted list to the dictionary\n",
    "    loadings[f'PC{i+1}'] = loading_list\n",
    "\n",
    "# Print out the loadings\n",
    "for pc, loading_list in loadings.items():\n",
    "    print(f'Principal Component {pc}:')\n",
    "    for var, loading in loading_list:\n",
    "        print(f'\\t{var}: {loading:.3f}')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "## Split the dataset into quantitative variables only\n",
    "\n",
    "# Using predefined quantitative columns, we need 3 PCs to cumulatively explain 80% of the variance\n",
    "num_vars = students_df_cleaned[quantitative_cols]\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "num_vars_scaled = scaler.fit_transform(num_vars)\n",
    "\n",
    "# Perform PCA on non-scaled variables\n",
    "pca = PCA()\n",
    "pca.fit(num_vars)\n",
    "\n",
    "# Get the eigenvalues and eigenvectors\n",
    "eigenvalues = pca.explained_variance_\n",
    "eigenvectors = pca.components_\n",
    "\n",
    "# Get the explained variance ratio for each principal component\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "\n",
    "# Create a figure with custom subplot grid\n",
    "fig = plt.figure(figsize=(15, 6))\n",
    "gs = gridspec.GridSpec(1, 3, width_ratios=[0.5, 0.5, 1])\n",
    "# Create the first subplot for explained variance ratio bar graph\n",
    "ax1 = plt.subplot(gs[0])\n",
    "ax1.bar(range(1, len(explained_variance_ratio)+1), explained_variance_ratio)\n",
    "ax1.set_xticks(range(1, len(explained_variance_ratio)+1))\n",
    "ax1.set_xlabel('Principal Component')\n",
    "ax1.set_ylabel('Explained Variance Ratio')\n",
    "ax1.set_title('Explained Variance Ratio for each Principal Component')\n",
    "\n",
    "# Create the second subplot for cumulative variance plot\n",
    "ax2 = plt.subplot(gs[1])\n",
    "cumulative_variance_ratio = np.cumsum(explained_variance_ratio)\n",
    "ax2.plot(range(1, len(cumulative_variance_ratio)+1,1), cumulative_variance_ratio)\n",
    "ax2.axhline(y=0.8, color='r', linestyle='-')\n",
    "variance_80 = next(idx for idx, cum_var in enumerate(pca.explained_variance_ratio_.cumsum()) if cum_var > 0.8)\n",
    "ax2.axvline(x=variance_80+1, color='r', linestyle='-')\n",
    "ax2.set_title('Cumulative Variance Plot')\n",
    "ax2.set_xlabel('Number of Principal Components')\n",
    "ax2.set_ylabel('Cumulative Explained Variance Ratio')\n",
    "\n",
    "# Fit PCA with 3 principal components based on the cumulative variance graph\n",
    "pca = PCA(n_components=3)\n",
    "pc_scores = pca.fit_transform(num_vars)\n",
    "\n",
    "# Define color scheme for each PC\n",
    "color_scheme = ['steelblue', 'darkorange', 'forestgreen', 'red', 'purple', 'gray', 'teal', 'coral', 'goldenrod', 'pink']\n",
    "\n",
    "# Create the third subplot for variance explained by each PC for each variable\n",
    "ax3 = plt.subplot(gs[2])\n",
    "for i in range(len(pca.explained_variance_ratio_)):\n",
    "    ax3.bar(num_vars.columns, pca.components_[i], label=f'PC{i+1}', color=color_scheme[i % len(color_scheme)])\n",
    "ax3.set_title('Variance Explained by first 3 PCs for each Variable')\n",
    "ax3.set_xlabel('Variables')\n",
    "ax3.set_ylabel('Variance Explained')\n",
    "ax3.legend()\n",
    "ax3.grid(axis='y')\n",
    "ax3.set_xticklabels(num_vars.columns, rotation=90, ha='right', wrap=True) # Wrap x-axis labels\n",
    "# Adjust the layout and spacing between subplots\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plots\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove quantitative variables from students_df_cleaned\n",
    "no_quantitative = students_df_cleaned.drop(quantitative_cols, axis=1)\n",
    "\n",
    "# Apply PCA to the remaining variables in no_quantitative, keeping only the first 3 principal components\n",
    "pca = PCA(n_components=3)\n",
    "pc_scores = pca.fit_transform(no_quantitative)\n",
    "\n",
    "# Convert pc_scores to a pandas DataFrame\n",
    "pc_df = pd.DataFrame(pc_scores, columns=['PC1', 'PC2', 'PC3'])\n",
    "\n",
    "# Concatenate pc_df with no_quantitative\n",
    "students_df_pca = pd.concat([no_quantitative, pc_df], axis=1)\n",
    "\n",
    "# View the new dataset\n",
    "students_df_pca.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove NaN values from the DataFrame\n",
    "students_df_pca = students_df_pca.dropna()\n",
    "\n",
    "# check for NaN values in the DataFrame\n",
    "nan_rows = students_df_pca.isnull().any(axis=1)\n",
    "\n",
    "# print the rows with NaN values\n",
    "print(students_df_pca[nan_rows])\n",
    "\n",
    "print(students_df_pca.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score\n",
    "from collections import defaultdict\n",
    "\n",
    "# Getting Data ready for Modelling\n",
    "X = students_df_cleaned.drop(\"Target\", axis=1)\n",
    "y = students_df_cleaned.Target\n",
    "\n",
    "# Create a dictionary of resampling methods and their corresponding datasets\n",
    "resampling_methods = {'Original': (X, y),\n",
    "                      'OverSampled': RandomOverSampler().fit_resample(X, y),\n",
    "                      'UnderSampled': RandomUnderSampler().fit_resample(X, y),\n",
    "                      'SMOTE': SMOTE().fit_resample(X, y)}\n",
    "\n",
    "# Create an empty list to store the model results for each resampling method\n",
    "model_results = []\n",
    "\n",
    "# Iterate through the resampling methods and their corresponding datasets\n",
    "for resampling_method, (X_resampled, y_resampled) in resampling_methods.items():\n",
    "    print('Resampling:', resampling_method)\n",
    "\n",
    "    # Model 1: Logistic Regression\n",
    "    logreg_model = LogisticRegression(max_iter=1000)\n",
    "    logreg_model.fit(X_resampled, y_resampled)\n",
    "    logreg_scores = cross_val_score(logreg_model, X_resampled, y_resampled, cv=5, scoring='accuracy')\n",
    "    logreg_precision, logreg_recall, logreg_f1, logreg_support = precision_recall_fscore_support(y_resampled, logreg_model.predict(X_resampled), average='weighted')\n",
    "    logreg_mean_score = np.mean(logreg_scores)\n",
    "    model_results.append({'Logistic ' + resampling_method: {'Accuracy': round(logreg_mean_score,4),\n",
    "                                                                      'Precision': round(logreg_precision,4),\n",
    "                                                                      'Recall': round(logreg_recall,4),\n",
    "                                                                      'F1': logreg_f1,\n",
    "                                                            'model': logreg_model}})\n",
    "\n",
    "    # Model 2: K-Nearest Neighbors\n",
    "    knn_model = KNeighborsClassifier()\n",
    "    knn_model.fit(X_resampled, y_resampled)\n",
    "    knn_scores = cross_val_score(knn_model, X_resampled, y_resampled, cv=5, scoring='accuracy')\n",
    "    knn_precision, knn_recall, knn_f1, knn_support = precision_recall_fscore_support(y_resampled, knn_model.predict(X_resampled), average='weighted')\n",
    "    knn_mean_score = np.mean(knn_scores)\n",
    "    model_results.append({'KNN ' + resampling_method: {'Accuracy': round(knn_mean_score,4),\n",
    "                                                                     'Precision': round(knn_precision,4),\n",
    "                                                                     'Recall': round(knn_recall,4),\n",
    "                                                                     'F1': knn_f1,\n",
    "                                                            'model': knn_model}})\n",
    "\n",
    "    # Model 3: Random Forest\n",
    "    rf_model = RandomForestClassifier()\n",
    "    rf_model.fit(X_resampled, y_resampled)\n",
    "    rf_scores = cross_val_score(rf_model, X_resampled, y_resampled, cv=5, scoring='accuracy')\n",
    "    rf_precision, rf_recall, rf_f1, rf_support = precision_recall_fscore_support(y_resampled, rf_model.predict(X_resampled), average='weighted')\n",
    "    rf_mean_score = np.mean(rf_scores)\n",
    "    model_results.append({'RF ' + resampling_method: {'Accuracy': round(rf_mean_score,4),\n",
    "                                                                     'Precision': round(rf_precision,4),\n",
    "                                                                     'Recall': round(rf_recall,4),\n",
    "                                                                     'F1': rf_scores,\n",
    "                                                            'model': rf_model}})\n",
    "\n",
    "    # Model 4: Decision Tree\n",
    "    dt_model = DecisionTreeClassifier()\n",
    "    dt_model.fit(X_resampled, y_resampled)\n",
    "    dt_scores = cross_val_score(dt_model, X_resampled, y_resampled, cv=5, scoring='accuracy')\n",
    "    dt_precision, dt_recall, dt_f1, dt_support = precision_recall_fscore_support(y_resampled, dt_model.predict(X_resampled), average='weighted')\n",
    "    dt_mean_score = np.mean(dt_scores)\n",
    "    model_results.append({'DT ' + resampling_method: {'Accuracy': round(dt_mean_score,4),\n",
    "                                                                'Precision': round(dt_precision,4),\n",
    "                                                                'Recall': round(dt_recall,4),\n",
    "                                                                'F1': dt_scores,\n",
    "                                                            'model': dt_model}})\n",
    "    # Model 5: SVM\n",
    "    svm_model = SVC()\n",
    "    svm_model.fit(X_resampled, y_resampled)\n",
    "    svm_scores = cross_val_score(svm_model, X_resampled, y_resampled, cv=5, scoring='accuracy')\n",
    "    svm_precision, svm_recall, svm_f1, svm_support = precision_recall_fscore_support(y_resampled, svm_model.predict(X_resampled), average='weighted')\n",
    "    svm_mean_score = np.mean(svm_scores)\n",
    "    model_results.append({'SVM ' + resampling_method: {'Accuracy': round(svm_mean_score,4),\n",
    "                                                    'Precision': round(svm_precision,4),\n",
    "                                                    'Recall': round(svm_recall,4),\n",
    "                                                    'F1': svm_scores,\n",
    "                                                    'model': svm_model}})\n",
    "\n",
    "# Display model performance metrics\n",
    "for result in model_results:\n",
    "    for model_name, scores in result.items():\n",
    "        print(f\"\\nResults for {model_name}:\")\n",
    "        for metric, value in scores.items():\n",
    "            print(f\"\\t{metric}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the model_results list by accuracy\n",
    "model_results_sorted = sorted(model_results, key=lambda x: x[list(x.keys())[0]]['Accuracy'], reverse=True)\n",
    "\n",
    "# Get the model names and their corresponding accuracy and precision scores\n",
    "model_names = []\n",
    "accuracy_scores = []\n",
    "precision_scores = []\n",
    "for model_dict in model_results_sorted:\n",
    "    model_name = list(model_dict.keys())[0]\n",
    "    model_names.append(model_name)\n",
    "    accuracy_scores.append(model_dict[model_name]['Accuracy'])\n",
    "    precision_scores.append(model_dict[model_name]['Precision'])\n",
    "\n",
    "# Plot the accuracy and precision bar charts side-by-side\n",
    "fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(16, 3))\n",
    "\n",
    "ax1.bar(model_names, accuracy_scores)\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.set_title(\"Accuracy Comparision\")\n",
    "ax1.set_xticks(range(len(model_names)))\n",
    "ax1.set_xticklabels(model_names, rotation=90)\n",
    "\n",
    "ax2.bar(model_names, precision_scores)\n",
    "ax2.set_ylabel('Precision')\n",
    "ax2.set_title(\"Precision Comparision\")\n",
    "ax2.set_xticks(range(len(model_names)))\n",
    "ax2.set_xticklabels(model_names, rotation=90)\n",
    "\n",
    "fig.suptitle('Model Performance (Cleaned Data with PCA)', fontsize=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the best performing model and its corresponding performance metrics\n",
    "best_model_name = list(model_results_sorted[0].keys())[0]\n",
    "best_model_performance = model_results_sorted[0][best_model_name]\n",
    "\n",
    "# Print the name of the best performing model\n",
    "print(f\"The best performing model is {best_model_name} with the following performance metrics:\")\n",
    "\n",
    "# Print the performance metrics of the best performing model\n",
    "for metric_name, metric_value in best_model_performance.items():\n",
    "    if metric_name != 'model':  # exclude the model from the printed performance metrics\n",
    "        print(f\"{metric_name}: {metric_value}\")\n",
    "    else:\n",
    "        best_model = metric_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "# Create an oversampler (e.g., using SMOTE)\n",
    "oversampler = RandomOverSampler()  # Your oversampler of choice\n",
    "\n",
    "rf = best_model\n",
    "\n",
    "# Create a StratifiedKFold cross-validator with 5 folds\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize an empty list to store confusion matrices for each fold\n",
    "confusion_matrices = []\n",
    "\n",
    "# Perform k-fold cross-validation\n",
    "for train_index, test_index in cv.split(X, y):\n",
    "    # Split data into training and test sets\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    # Oversample the training data\n",
    "    X_train_oversampled, y_train_oversampled = oversampler.fit_resample(X_train, y_train)\n",
    "\n",
    "    # Train the Random Forest classifier on the oversampled training data\n",
    "    #rf.fit(X_train_oversampled, y_train_oversampled)\n",
    "\n",
    "    # Predict labels on the test data\n",
    "    y_pred = rf.predict(X_test)\n",
    "\n",
    "    # Compute the confusion matrix for the current fold and append to the list\n",
    "    confusion_matrix_fold = confusion_matrix(y_test, y_pred)\n",
    "    confusion_matrices.append(confusion_matrix_fold)\n",
    "# Print the confusion matrices for each fold\n",
    "for i, confusion_matrix_fold in enumerate(confusion_matrices):\n",
    "    print(f\"Confusion matrix for fold {i+1}:\")\n",
    "    print(confusion_matrix_fold)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the best performing model and it's corresponding accuracy\n",
    "best_model = model_results_sorted[0][list(model_results_sorted[0].keys())[0]]['model']\n",
    "best_model_accuracy = model_results_sorted[0][list(model_results_sorted[0].keys())[0]]['Accuracy']\n",
    "\n",
    "# Print the best performing model's name and accuracy\n",
    "print(f\"The best performing model is {list(model_results_sorted[0].keys())[0]} with accuracy {best_model_accuracy}\")\n",
    "\n",
    "# get feature importances of RandomOverSampler balanced model\n",
    "feature_importances = best_model.feature_importances_\n",
    "\n",
    "# create a dataframe of feature importances\n",
    "importances_df = pd.DataFrame({'feature': X.columns, 'importance': feature_importances})\n",
    "\n",
    "# sort the dataframe by feature importance in descending order\n",
    "importances_df = importances_df.sort_values('importance', ascending=False)\n",
    "\n",
    "# print the top 10 features with their importances\n",
    "print(importances_df.head(10))\n",
    "\n",
    "# Set plot size to a square\n",
    "plt.figure(figsize=(6, 6))\n",
    "\n",
    "# create a bar chart of feature importances for the top 10 features\n",
    "top_features = importances_df.head(10)['feature'].values\n",
    "top_importances = importances_df.head(10)['importance'].values\n",
    "sorted_idx = top_importances.argsort()\n",
    "plt.barh(range(len(top_importances)), top_importances[sorted_idx])\n",
    "plt.yticks(range(len(top_importances)), top_features[sorted_idx], wrap=True) # Wrap text\n",
    "plt.title(\"Random Forest (Oversampled)\\nwith Cleaned Data\")\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from collections import defaultdict\n",
    "\n",
    "# Getting Data ready for Modelling\n",
    "X = students_df_pca.drop(\"Target\", axis=1)\n",
    "y = students_df_pca.Target\n",
    "\n",
    "# Create a dictionary of resampling methods and their corresponding datasets\n",
    "resampling_methods = {'Original': (X, y),\n",
    "                      'OverSampled': RandomOverSampler().fit_resample(X, y),\n",
    "                      'UnderSampled': RandomUnderSampler().fit_resample(X, y),\n",
    "                      'SMOTE': SMOTE().fit_resample(X, y)}\n",
    "\n",
    "# Create an empty list to store the model results for each resampling method\n",
    "model_results = []\n",
    "\n",
    "# Iterate through the resampling methods and their corresponding datasets\n",
    "for resampling_method, (X_resampled, y_resampled) in resampling_methods.items():\n",
    "    print('Resampling:', resampling_method)\n",
    "\n",
    "    # Model 1: Logistic Regression\n",
    "    logreg_model = LogisticRegression(max_iter=1000)\n",
    "    logreg_model.fit(X_resampled, y_resampled)\n",
    "    logreg_scores = cross_val_score(logreg_model, X_resampled, y_resampled, cv=5, scoring='accuracy')\n",
    "    logreg_precision, logreg_recall, logreg_f1, logreg_support = precision_recall_fscore_support(y_resampled, logreg_model.predict(X_resampled), average='weighted')\n",
    "    logreg_mean_score = np.mean(logreg_scores)\n",
    "    model_results.append({'Logistic ' + resampling_method: {'Accuracy': round(logreg_mean_score,2),\n",
    "                                                                      'Precision': round(logreg_precision,2),\n",
    "                                                                      'Recall': round(logreg_recall,2),\n",
    "                                                                      'F1': logreg_f1,\n",
    "                                                            'model': logreg_model}})\n",
    "\n",
    "    # Model 2: K-Nearest Neighbors\n",
    "    knn_model = KNeighborsClassifier()\n",
    "    knn_model.fit(X_resampled, y_resampled)\n",
    "    knn_scores = cross_val_score(knn_model, X_resampled, y_resampled, cv=5, scoring='accuracy')\n",
    "    knn_precision, knn_recall, knn_f1, knn_support = precision_recall_fscore_support(y_resampled, knn_model.predict(X_resampled), average='weighted')\n",
    "    knn_mean_score = np.mean(knn_scores)\n",
    "    model_results.append({'KNN ' + resampling_method: {'Accuracy': round(knn_mean_score,2),\n",
    "                                                                     'Precision': round(knn_precision,2),\n",
    "                                                                     'Recall': round(knn_recall,2),\n",
    "                                                                     'F1': knn_f1,\n",
    "                                                            'model': knn_model}})\n",
    "\n",
    "    # Model 3: Random Forest\n",
    "    rf_model = RandomForestClassifier()\n",
    "    rf_model.fit(X_resampled, y_resampled)\n",
    "    rf_scores = cross_val_score(rf_model, X_resampled, y_resampled, cv=5, scoring='accuracy')\n",
    "    rf_precision, rf_recall, rf_f1, rf_support = precision_recall_fscore_support(y_resampled, rf_model.predict(X_resampled), average='weighted')\n",
    "    rf_mean_score = np.mean(rf_scores)\n",
    "    model_results.append({'RF ' + resampling_method: {'Accuracy': round(rf_mean_score,2),\n",
    "                                                                     'Precision': round(rf_precision,2),\n",
    "                                                                     'Recall': round(rf_recall,2),\n",
    "                                                                     'F1': rf_scores,\n",
    "                                                            'model': rf_model}})\n",
    "    # Model 4: Decision Tree\n",
    "    dt_model = DecisionTreeClassifier()\n",
    "    dt_model.fit(X_resampled, y_resampled)\n",
    "    dt_scores = cross_val_score(dt_model, X_resampled, y_resampled, cv=5, scoring='accuracy')\n",
    "    dt_precision, dt_recall, dt_f1, dt_support = precision_recall_fscore_support(y_resampled, dt_model.predict(X_resampled), average='weighted')\n",
    "    dt_mean_score = np.mean(dt_scores)\n",
    "    model_results.append({'DT ' + resampling_method: {'Accuracy': round(dt_mean_score,2),\n",
    "                                                                'Precision': round(dt_precision,2),\n",
    "                                                                'Recall': round(dt_recall,2),\n",
    "                                                                'F1': dt_scores,\n",
    "                                                            'model': dt_model}})\n",
    "    # Model 5: SVM\n",
    "    svm_model = SVC()\n",
    "    svm_model.fit(X_resampled, y_resampled)\n",
    "    svm_scores = cross_val_score(svm_model, X_resampled, y_resampled, cv=5, scoring='accuracy')\n",
    "    svm_precision, svm_recall, svm_f1, svm_support = precision_recall_fscore_support(y_resampled, svm_model.predict(X_resampled), average='weighted')\n",
    "    svm_mean_score = np.mean(svm_scores)\n",
    "    model_results.append({'SVM ' + resampling_method: {'Accuracy': round(svm_mean_score,2),\n",
    "                                                    'Precision': round(svm_precision,2),\n",
    "                                                    'Recall': round(svm_recall,2),\n",
    "                                                    'F1': svm_scores,\n",
    "                                                    'model': svm_model}})\n",
    "\n",
    "# Display model performance metrics\n",
    "for result in model_results:\n",
    "    for model_name, scores in result.items():\n",
    "        print(f\"\\nResults for {model_name}:\")\n",
    "        for metric, value in scores.items():\n",
    "            print(f\"\\t{metric}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the model_results list by accuracy\n",
    "model_results_sorted = sorted(model_results, key=lambda x: x[list(x.keys())[0]]['Accuracy'], reverse=True)\n",
    "\n",
    "# Get the model names and their corresponding accuracy and precision scores\n",
    "model_names = []\n",
    "accuracy_scores = []\n",
    "precision_scores = []\n",
    "for model_dict in model_results_sorted:\n",
    "    model_name = list(model_dict.keys())[0]\n",
    "    model_names.append(model_name)\n",
    "    accuracy_scores.append(model_dict[model_name]['Accuracy'])\n",
    "    precision_scores.append(model_dict[model_name]['Precision'])\n",
    "\n",
    "# Plot the accuracy and precision bar charts side-by-side\n",
    "fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(16, 3))\n",
    "\n",
    "ax1.bar(model_names, accuracy_scores)\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.set_title(\"Accuracy Comparision\")\n",
    "ax1.set_xticks(range(len(model_names)))\n",
    "ax1.set_xticklabels(model_names, rotation=90)\n",
    "\n",
    "ax2.bar(model_names, precision_scores)\n",
    "ax2.set_ylabel('Precision')\n",
    "ax2.set_title(\"Precision Comparision\")\n",
    "ax2.set_xticks(range(len(model_names)))\n",
    "ax2.set_xticklabels(model_names, rotation=90)\n",
    "\n",
    "fig.suptitle('Model Performance (Cleaned Data with PCA)', fontsize=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the best performing model and its corresponding performance metrics\n",
    "best_model_name = list(model_results_sorted[0].keys())[0]\n",
    "best_model_performance = model_results_sorted[0][best_model_name]\n",
    "\n",
    "# Print the name of the best performing model\n",
    "print(f\"The best performing model is {best_model_name} with the following performance metrics:\")\n",
    "\n",
    "# Print the performance metrics of the best performing model\n",
    "for metric_name, metric_value in best_model_performance.items():\n",
    "    if metric_name != 'model':  # exclude the model from the printed performance metrics\n",
    "        print(f\"{metric_name}: {metric_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the best performing model and it's corresponding accuracy\n",
    "best_model = model_results_sorted[0][list(model_results_sorted[0].keys())[0]]['model']\n",
    "best_model_accuracy = model_results_sorted[0][list(model_results_sorted[0].keys())[0]]['Accuracy']\n",
    "\n",
    "# Print the best performing model's name and accuracy\n",
    "print(f\"The best performing model is {list(model_results_sorted[0].keys())[0]} with accuracy {best_model_accuracy}\")\n",
    "\n",
    "# get feature importances of RandomOverSampler balanced model\n",
    "feature_importances = best_model.feature_importances_\n",
    "\n",
    "# create a dataframe of feature importances\n",
    "importances_df = pd.DataFrame({'feature': X.columns, 'importance': feature_importances})\n",
    "\n",
    "# sort the dataframe by feature importance in descending order\n",
    "importances_df = importances_df.sort_values('importance', ascending=False)\n",
    "\n",
    "# print the top 10 features with their importances\n",
    "print(importances_df.head(10))\n",
    "\n",
    "# Set plot size to a square\n",
    "plt.figure(figsize=(6, 6))\n",
    "\n",
    "# create a bar chart of feature importances for the top 10 features\n",
    "top_features = importances_df.head(10)['feature'].values\n",
    "top_importances = importances_df.head(10)['importance'].values\n",
    "sorted_idx = top_importances.argsort()\n",
    "plt.barh(range(len(top_importances)), top_importances[sorted_idx])\n",
    "plt.yticks(range(len(top_importances)), top_features[sorted_idx], wrap=True) # Wrap text\n",
    "plt.title(\"Random Forest (Oversampled)\\nwith Cleaned Data\")\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Target to numerical\n",
    "students_df_normalised_no_outliers[\"Target\"].replace('Dropout', 0, inplace=True)\n",
    "students_df_normalised_no_outliers[\"Target\"].replace('Graduate', 1, inplace=True)\n",
    "students_df_normalised_no_outliers[\"Target\"].replace('Enrolled', 2, inplace=True)\n",
    "\n",
    "# Removing enrolled from Target\n",
    "students_df_normalised_no_outliers = students_df_normalised_no_outliers[students_df_normalised_no_outliers['Target'] != 2]\n",
    "\n",
    "# Define features to remove\n",
    "features_to_remove = [\"Curricular units 1st sem (credited)\", \"Curricular units 1st sem (enrolled)\",\n",
    "                      \"Curricular units 1st sem (evaluations)\", \"Curricular units 1st sem (approved)\",\n",
    "                      \"Curricular units 1st sem (grade)\", \"Curricular units 2nd sem (approved)\", \"Nationality\"]\n",
    "\n",
    "# remove the specified columns from the dataframe\n",
    "students_df_normalised_no_outliers = students_df_normalised_no_outliers.drop(features_to_remove, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from collections import defaultdict\n",
    "\n",
    "# Getting Data ready for Modelling\n",
    "X = students_df_normalised_no_outliers.drop(\"Target\", axis=1)\n",
    "y = students_df_normalised_no_outliers.Target\n",
    "\n",
    "# Create a dictionary of resampling methods and their corresponding datasets\n",
    "resampling_methods = {'Original': (X, y),\n",
    "                      'OverSampled': RandomOverSampler().fit_resample(X, y),\n",
    "                      'UnderSampled': RandomUnderSampler().fit_resample(X, y),\n",
    "                      'SMOTE': SMOTE().fit_resample(X, y)}\n",
    "\n",
    "# Create an empty list to store the model results for each resampling method\n",
    "model_results_2 = []\n",
    "\n",
    "# Iterate through the resampling methods and their corresponding datasets\n",
    "for resampling_method, (X_resampled, y_resampled) in resampling_methods.items():\n",
    "    print('Resampling:', resampling_method)\n",
    "\n",
    "\n",
    "    # Model 1: Logistic Regression\n",
    "    logreg_model = LogisticRegression(max_iter=1000)\n",
    "    logreg_model.fit(X_resampled, y_resampled)\n",
    "    logreg_scores = cross_val_score(logreg_model, X_resampled, y_resampled, cv=5, scoring='accuracy')\n",
    "    logreg_precision, logreg_recall, logreg_f1, logreg_support = precision_recall_fscore_support(y_resampled, logreg_model.predict(X_resampled), average='weighted')\n",
    "    logreg_mean_score = np.mean(logreg_scores)\n",
    "    model_results_2.append({'Logistic ' + resampling_method: {'Accuracy': round(logreg_mean_score,2),\n",
    "                                                                      'Precision': round(logreg_precision,2),\n",
    "                                                                      'Recall': round(logreg_recall,2),\n",
    "                                                                      'F1': logreg_f1,\n",
    "                                                            'model': logreg_model}})\n",
    "\n",
    "    # Model 2: K-Nearest Neighbors\n",
    "    knn_model = KNeighborsClassifier()\n",
    "    knn_model.fit(X_resampled, y_resampled)\n",
    "    knn_scores = cross_val_score(knn_model, X_resampled, y_resampled, cv=5, scoring='accuracy')\n",
    "    knn_precision, knn_recall, knn_f1, knn_support = precision_recall_fscore_support(y_resampled, knn_model.predict(X_resampled), average='weighted')\n",
    "    knn_mean_score = np.mean(knn_scores)\n",
    "    model_results_2.append({'KNN ' + resampling_method: {'Accuracy': round(knn_mean_score,2),\n",
    "                                                                     'Precision': round(knn_precision,2),\n",
    "                                                                     'Recall': round(knn_recall,2),\n",
    "                                                                     'F1': knn_f1,\n",
    "                                                            'model': knn_model}})\n",
    "\n",
    "    # Model 3: Random Forest\n",
    "    rf_model = RandomForestClassifier()\n",
    "    rf_model.fit(X_resampled, y_resampled)\n",
    "    rf_scores = cross_val_score(rf_model, X_resampled, y_resampled, cv=5, scoring='accuracy')\n",
    "    rf_precision, rf_recall, rf_f1, rf_support = precision_recall_fscore_support(y_resampled, rf_model.predict(X_resampled), average='weighted')\n",
    "    rf_mean_score = np.mean(rf_scores)\n",
    "    model_results_2.append({'RF ' + resampling_method: {'Accuracy': round(rf_mean_score,2),\n",
    "                                                                     'Precision': round(rf_precision,2),\n",
    "                                                                     'Recall': round(rf_recall,2),\n",
    "                                                                     'F1': rf_scores,\n",
    "                                                            'model': rf_model}})\n",
    "    # Model 4: Decision Tree\n",
    "    dt_model = DecisionTreeClassifier()\n",
    "    dt_model.fit(X_resampled, y_resampled)\n",
    "    dt_scores = cross_val_score(dt_model, X_resampled, y_resampled, cv=5, scoring='accuracy')\n",
    "    dt_precision, dt_recall, dt_f1, dt_support = precision_recall_fscore_support(y_resampled, dt_model.predict(X_resampled), average='weighted')\n",
    "    dt_mean_score = np.mean(dt_scores)\n",
    "    model_results_2.append({'DT ' + resampling_method: {'Accuracy': round(dt_mean_score,2),\n",
    "                                                                'Precision': round(dt_precision,2),\n",
    "                                                                'Recall': round(dt_recall,2),\n",
    "                                                                'F1': dt_scores,\n",
    "                                                            'model': dt_model}})\n",
    "    # Model 5: SVM\n",
    "    svm_model = SVC()\n",
    "    svm_model.fit(X_resampled, y_resampled)\n",
    "    svm_scores = cross_val_score(svm_model, X_resampled, y_resampled, cv=5, scoring='accuracy')\n",
    "    svm_precision, svm_recall, svm_f1, svm_support = precision_recall_fscore_support(y_resampled, svm_model.predict(X_resampled), average='weighted')\n",
    "    svm_mean_score = np.mean(svm_scores)\n",
    "    model_results_2.append({'SVM ' + resampling_method: {'Accuracy': round(svm_mean_score,2),\n",
    "                                                    'Precision': round(svm_precision,2),\n",
    "                                                    'Recall': round(svm_recall,2),\n",
    "                                                    'F1': svm_scores,\n",
    "                                                    'model': svm_model}})\n",
    "\n",
    "# Display model performance metrics\n",
    "for result in model_results_2:\n",
    "    for model_name, scores in result.items():\n",
    "        print(f\"\\nResults for {model_name}:\")\n",
    "        for metric, value in scores.items():\n",
    "            print(f\"\\t{metric}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the model_results list by accuracy\n",
    "model_results_sorted_2 = sorted(model_results_2, key=lambda x: x[list(x.keys())[0]]['Accuracy'], reverse=True)\n",
    "\n",
    "# Get the model names and their corresponding accuracy and precision scores\n",
    "model_names = []\n",
    "accuracy_scores = []\n",
    "precision_scores = []\n",
    "for model_dict in model_results_sorted_2:\n",
    "    model_name = list(model_dict.keys())[0]\n",
    "    model_names.append(model_name)\n",
    "    accuracy_scores.append(model_dict[model_name]['Accuracy'])\n",
    "    precision_scores.append(model_dict[model_name]['Precision'])\n",
    "\n",
    "# Plot the accuracy and precision bar charts side-by-side\n",
    "fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(16, 3))\n",
    "\n",
    "ax1.bar(model_names, accuracy_scores)\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.set_title(\"Accuracy Comparision\", fontsize=11)\n",
    "ax1.set_xticks(range(len(model_names)))\n",
    "ax1.set_xticklabels(model_names, rotation=90)\n",
    "\n",
    "ax2.bar(model_names, precision_scores)\n",
    "ax2.set_ylabel('Precision')\n",
    "ax2.set_title(\"Precision Comparision\", fontsize=11)\n",
    "ax2.set_xticks(range(len(model_names)))\n",
    "ax2.set_xticklabels(model_names, rotation=90)\n",
    "\n",
    "fig.suptitle('Model Performance (Normalized, No Outliers)', fontsize=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the best performing model and its corresponding performance metrics\n",
    "best_model_name = list(model_results_sorted_2[0].keys())[0]\n",
    "best_model_performance = model_results_sorted_2[0][best_model_name]\n",
    "\n",
    "# Print the name of the best performing model\n",
    "print(f\"The best performing model is {best_model_name} with the following performance metrics:\")\n",
    "\n",
    "# Print the performance metrics of the best performing model\n",
    "for metric_name, metric_value in best_model_performance.items():\n",
    "    if metric_name != 'model':  # exclude the model from the printed performance metrics\n",
    "        print(f\"{metric_name}: {metric_value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the best performing model and it's corresponding accuracy\n",
    "best_model = model_results_sorted_2[0][list(model_results_sorted_2[0].keys())[0]]['model']\n",
    "best_model_accuracy = model_results_sorted_2[0][list(model_results_sorted_2[0].keys())[0]]['Accuracy']\n",
    "\n",
    "# Print the best performing model's name and accuracy\n",
    "print(f\"The best performing model is {list(model_results_sorted_2[0].keys())[0]} with accuracy {best_model_accuracy}\")\n",
    "\n",
    "# get feature importances of RandomOverSampler balanced model\n",
    "feature_importances = best_model.feature_importances_\n",
    "\n",
    "# create a dataframe of feature importances\n",
    "importances_df = pd.DataFrame({'feature': X.columns, 'importance': feature_importances})\n",
    "\n",
    "# sort the dataframe by feature importance in descending order\n",
    "importances_df = importances_df.sort_values('importance', ascending=False)\n",
    "\n",
    "# print the top 10 features with their importances\n",
    "print(importances_df.head(10))\n",
    "\n",
    "# Set plot size to a square\n",
    "plt.figure(figsize=(6, 6))\n",
    "\n",
    "# Create a bar chart of feature importances for the top 10 features\n",
    "top_features = importances_df.head(10)['feature'].values\n",
    "top_importances = importances_df.head(10)['importance'].values\n",
    "sorted_idx = top_importances.argsort()\n",
    "plt.barh(range(len(top_importances)), top_importances[sorted_idx])\n",
    "plt.yticks(range(len(top_importances)), top_features[sorted_idx], wrap=True) # Wrap text\n",
    "plt.title(\"Random Forest (Oversampled)\\nwith Additionally Normalised and Outliers Removed Data\")\n",
    "plt.xlabel('Feature Importance')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "student_dropout",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
